{   
    "alg": {
        "alg": "PPO"
    },
    "model": {
        "policy":               "MlpPolicy",
        "env":                  "CartPole-v1",
        "learning_rate":        3e-4,
        "n_steps":              2048,
        "batch_size":           64,
        "n_epochs":             1,
        "gamma":                0.99,
        "gae_lambda":           0.95,
        "clip_range":           0.2,
        "clip_range_vf":        "None",
        "ent_coef":             0.0,
        "vf_coef":              0.5,
        "max_grad_norm":        0.5,
        "use_sde":              "False",
        "sde_sample_freq":      -1,
        "target_kl":            "None",
        "tensorboard_log":      "None", 
        "create_eval_env":      "False",
        "policy_kwargs":        "dict(activation_fn=th.nn.ReLU, net_arch=[dict(pi=[32, 32], vf=[32, 32])])",
        "verbose":              1,
        "seed":                 "None",   
        "device":               "auto", 
        "_init_setup_model":    "True"
    },
    "training": {
        "total_timesteps":      10000,
        "callback":             "None",
        "log_interval":         1,
        "eval_env":             "None",
        "eval_freq":            -1,
        "n_eval_episodes":      1,
        "tb_log_name":          "PPO",
        "eval_log_path":        "None",
        "reset_num_timesteps":  "True"
    }
}